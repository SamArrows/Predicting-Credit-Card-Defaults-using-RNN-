{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "gpwg7V13XhZ6",
        "outputId": "5e3034ca-9d09-40ab-b1c5-4b11e5672384"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8fba4ce8-93a7-44f7-a9a1-cc4ef71e7ae2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8fba4ce8-93a7-44f7-a9a1-cc4ef71e7ae2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CCD.xls to CCD.xls\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nL1 Normalization: normalizes so that the sum of absolute values along the specified axis is 1\\nRecommended for:\\n- Feature selection in sparse machine learning models\\n- When outliers are present and should not be removed\\n\\nL2 Normalization: normalizes so that the Euclidean norm (L2 norm) along the specified axis is 1\\nRecommended for:\\n- When the direction of the data matters more than the actual values\\n- Regularization techniques in machine learning, such as weight decay\\nSometimes recommended to apply L2 after applying Z standardization which centers the data around 0 with a standard deviation of 1. Here are the reasons for experimenting with it:\\n-Normalization for Specific Model Requirements: Some models or algorithms might benefit from having input data normalized in a specific way. Applying L2 normalization after Z-Standardization can be a part of such model-specific data preparation.\\n- Feature Scaling: L2 normalization can scale the feature vectors to have a Euclidean norm of 1, which might be beneficial for certain models that rely on the magnitude of the feature vector.\\n- Regularization: L2 normalization can serve as a form of regularization by penalizing large weights in the model, leading to improved generalization and potentially preventing overfitting.\\n- Enhanced Numerical Stability: In some cases, combining Z-Standardization with L2 normalization can help improve the numerical stability of the optimization process during training.\\n- Experimental Purposes: If you are experimenting with different normalization techniques to see how they affect your model's performance, applying L2 normalization after Z-Standardization could be part of your experimentation process.\\n\\nMin-Max Normalization: Scales the data to a fixed range (e.g., 0 to 1).\\nRecommended for temporal data:\\nMin-Max normalization is often suitable for temporal data when you want to preserve the temporal relationships in the data while scaling it to a specific range.\\nUseful when the absolute values of the data are not as important as the relative relationships between data points over time.\\n\\n\\nZ-Score Normalization (Standardization):\\nMethod: Centers the data around 0 with a standard deviation of 1.\\nRecommended for temporal data:\\nZ-score normalization is beneficial when the temporal data follows a Gaussian distribution.\\nIt can help in dealing with temporal data that exhibits seasonality or trends by removing the mean and scaling by the standard deviation.\\n\\nLSTM Normalization (Layer Normalization):\\nMethod: Normalizes the activations of each time step in a sequence.\\nRecommended for temporal data:\\nSpecifically designed for recurrent neural networks like LSTMs (Long Short-Term Memory networks) to normalize the hidden states at each time step.\\nHelps in stabilizing training and improving convergence in sequential models.\\n\\nFeature Scaling based on Time Windows:\\nMethod: Normalize features within specific time windows or segments.\\nRecommended for temporal data:\\nDivide the temporal data into windows and scale the features within each window independently.\\nUseful when different parts of the time series have varying characteristics or scales.\\n\\nDifferencing:\\nMethod: Compute differences between consecutive time steps.\\nRecommended for temporal data:\\nTransform the data by taking differences between adjacent time points to remove trends or seasonality.\\nOften used in time series analysis to make the data stationary.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#use this to select the CCD.xls file and upload to drive; the following code block will then successfully be able to open it\n",
        "#https://saturncloud.io/blog/how-to-read-a-file-from-drive-in-google-colab/\n",
        "'''\n",
        "L1 Normalization: normalizes so that the sum of absolute values along the specified axis is 1\n",
        "Recommended for:\n",
        "- Feature selection in sparse machine learning models\n",
        "- When outliers are present and should not be removed\n",
        "\n",
        "L2 Normalization: normalizes so that the Euclidean norm (L2 norm) along the specified axis is 1\n",
        "Recommended for:\n",
        "- When the direction of the data matters more than the actual values\n",
        "- Regularization techniques in machine learning, such as weight decay\n",
        "Sometimes recommended to apply L2 after applying Z standardization which centers the data around 0 with a standard deviation of 1. Here are the reasons for experimenting with it:\n",
        "-Normalization for Specific Model Requirements: Some models or algorithms might benefit from having input data normalized in a specific way. Applying L2 normalization after Z-Standardization can be a part of such model-specific data preparation.\n",
        "- Feature Scaling: L2 normalization can scale the feature vectors to have a Euclidean norm of 1, which might be beneficial for certain models that rely on the magnitude of the feature vector.\n",
        "- Regularization: L2 normalization can serve as a form of regularization by penalizing large weights in the model, leading to improved generalization and potentially preventing overfitting.\n",
        "- Enhanced Numerical Stability: In some cases, combining Z-Standardization with L2 normalization can help improve the numerical stability of the optimization process during training.\n",
        "- Experimental Purposes: If you are experimenting with different normalization techniques to see how they affect your model's performance, applying L2 normalization after Z-Standardization could be part of your experimentation process.\n",
        "\n",
        "Min-Max Normalization: Scales the data to a fixed range (e.g., 0 to 1).\n",
        "Recommended for temporal data:\n",
        "Min-Max normalization is often suitable for temporal data when you want to preserve the temporal relationships in the data while scaling it to a specific range.\n",
        "Useful when the absolute values of the data are not as important as the relative relationships between data points over time.\n",
        "\n",
        "\n",
        "Z-Score Normalization (Standardization):\n",
        "Method: Centers the data around 0 with a standard deviation of 1.\n",
        "Recommended for temporal data:\n",
        "Z-score normalization is beneficial when the temporal data follows a Gaussian distribution.\n",
        "It can help in dealing with temporal data that exhibits seasonality or trends by removing the mean and scaling by the standard deviation.\n",
        "\n",
        "LSTM Normalization (Layer Normalization):\n",
        "Method: Normalizes the activations of each time step in a sequence.\n",
        "Recommended for temporal data:\n",
        "Specifically designed for recurrent neural networks like LSTMs (Long Short-Term Memory networks) to normalize the hidden states at each time step.\n",
        "Helps in stabilizing training and improving convergence in sequential models.\n",
        "\n",
        "Feature Scaling based on Time Windows:\n",
        "Method: Normalize features within specific time windows or segments.\n",
        "Recommended for temporal data:\n",
        "Divide the temporal data into windows and scale the features within each window independently.\n",
        "Useful when different parts of the time series have varying characteristics or scales.\n",
        "\n",
        "Differencing:\n",
        "Method: Compute differences between consecutive time steps.\n",
        "Recommended for temporal data:\n",
        "Transform the data by taking differences between adjacent time points to remove trends or seasonality.\n",
        "Often used in time series analysis to make the data stationary.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "df = pd.read_excel('/content/CCD.xls')\n",
        "input = np.delete(df.to_numpy(dtype = None, copy = False), 0, 0) #convert dataframe to numpy array and delete the headers\n",
        "\n",
        "data_tensor = torch.from_numpy(input.astype('float64'))\n",
        "#print(data_tensor)\n",
        "\n",
        "'''\n",
        "We need to normalize the data differently depending on which features will be used\n",
        "    for their temporal dependencies (like payments each month) and which features won't (such as sex and age)\n",
        "    hence we need to split the data accordingly\n",
        "\n",
        "Each set of temporal data will be normalized separate to each other and the non-temporal data but will use the same normalization technique - minimax\n",
        "Non-temporal data will be normalized together using Z-standardization ==> experimentation with applying L2 normalization after Z-standardization is recommended\n",
        "'''\n",
        "\n",
        "class WindowSizeGreaterThanSequenceException(Exception):\n",
        "    def __init__(self, message=\"The size of the window defined for each subsequence is larger than the input sequence itself - it must be smaller than it\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class OverlapSizeException(Exception):\n",
        "    def __init__(self, message=\"The size of the overlap is not between 0 and 1\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class OverlapTypeException(Exception):\n",
        "    def __init__(self, message=\"overlap type must be a float\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class WindowTypeException(Exception):\n",
        "    def __init__(self, message=\"window_size type must be int\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class InputListTypeException(Exception):\n",
        "    def __init__(self, message=\"input_list must be of type list\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "\n",
        "def z_standardize(input_tensor):\n",
        "  mean = input_tensor.mean()  # Calculate mean and standard deviation\n",
        "  std = input_tensor.std()\n",
        "  return (input_tensor - mean) / std  # Manually apply Z-Standardization\n",
        "\n",
        "def minmax_normalization(input_tensor):\n",
        "  min_val = input_tensor.min()   # Define the min and max values for normalization\n",
        "  max_val = input_tensor.max()\n",
        "  return (input_tensor - min_val) / (max_val - min_val)  # Apply Min-Max normalization\n",
        "\n",
        "def window_overlap(input_list: list, window_size: int = 3, overlap: float = 0.7):\n",
        "  '''\n",
        "  - decide how big each window is and how much overlap it shares with the previous window\n",
        "    - overlap should be written as a decimal, i.e. 0.5 = 50% overlap, and between 0 and 1\n",
        "    - window_size can't be larger than the entire input list\n",
        "    - both overlap and window_size need to be numbers ==> window_size = int, overlap = float\n",
        "  - create a new list with the windows within it as subsequences per say\n",
        "  '''\n",
        "  try:\n",
        "      if isinstance(window_size, int):\n",
        "        if isinstance(input_list, list):\n",
        "          if window_size < len(input_list):\n",
        "            if isinstance(overlap, float):\n",
        "\n",
        "              if 0 < overlap < 1:\n",
        "                  # the overlap percentage needs to be able to divide the list into integer-sized subdivisions ==> we can use integer division and if the number doesn't divide perfectly, one of the windows can be smaller\n",
        "                  # example ==> [1, 2, 3, 4, 5, 6] split into 3 windows with 50% overlap = [[1, 2, 3], [3, 4, 5], [5, 6]]\n",
        "                  # example ==> [1, 2, 3, 4, 5, 6] split into windows of size 3 with 70% overlap = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]] as the next best situation is with 2/3 overlap\n",
        "\n",
        "                  elements_shared_per_window = math.floor(window_size * overlap)\n",
        "                  elements_different_to_prev_window = window_size - elements_shared_per_window\n",
        "\n",
        "                  windows =  []\n",
        "                  number_of_windows = (len(input_list) // elements_shared_per_window)+1\n",
        "                  for i in range(0, number_of_windows):\n",
        "                    windows.append([])\n",
        "\n",
        "                  last_index = window_size\n",
        "                  windows[0] = input_list[0:last_index]\n",
        "\n",
        "                  all_filled = False\n",
        "\n",
        "                  for i in range(1, len(windows)):\n",
        "                    if not all_filled:\n",
        "                      for j in range(-1, -(elements_shared_per_window+1), -1):\n",
        "                        windows[i].append(windows[i-1][j])\n",
        "                      windows[i].reverse()\n",
        "\n",
        "                      for element in input_list[last_index:last_index+elements_different_to_prev_window]:\n",
        "                        windows[i].append(element)\n",
        "                      last_index += elements_different_to_prev_window\n",
        "                      if last_index >= len(input_list):\n",
        "                        all_filled = True\n",
        "                  return [sublist for sublist in windows if sublist]  # ensures no empty lists are left over if the division into sublists is not perfect\n",
        "\n",
        "              else:\n",
        "                raise OverlapSizeException\n",
        "            else:\n",
        "              raise OverlapTypeException\n",
        "          else:\n",
        "            raise WindowSizeGreaterThanSequenceException\n",
        "        else:\n",
        "          raise InputListTypeException\n",
        "      else:\n",
        "        raise WindowTypeException\n",
        "  except WindowTypeException as e:\n",
        "      print(e)\n",
        "  except InputListTypeException as e:\n",
        "      print(e)\n",
        "  except WindowSizeGreaterThanSequenceException as e:\n",
        "      print(e)\n",
        "  except OverlapTypeException as e:\n",
        "      print(e)\n",
        "  except OverlapSizeException as e:\n",
        "      print(e)\n",
        "\n",
        "def window_tensor(input_tensor: torch.tensor, window_size: int, overlap: float):\n",
        "  # applies the window overlap algorithm to a tensor\n",
        "  return torch.tensor([window_overlap(lst) for lst in input_tensor.tolist()])\n"
      ],
      "metadata": {
        "id": "k5EHzPJaXu6r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pay = data_tensor[:, 5:11]\n",
        "bill_amt = data_tensor[:, 11:17]\n",
        "pay_amt = data_tensor[:, 17:23]\n",
        "non_temp = data_tensor[:, 0:5]\n",
        "\n",
        "labels = data_tensor[:, -1].unsqueeze(1)  # Extract the last column and add a new dimension\n",
        "\n",
        "z_non_temp = z_standardize(non_temp)\n",
        "\n",
        "# Apply L2 normalization using F.normalize\n",
        "z_non_temp_norm = F.normalize(z_non_temp, p=2, dim=1)  # L2 normalization can be experimented with in combination with Z-standardization\n",
        "pay_norm = minmax_normalization(pay)\n",
        "bill_amt_norm = minmax_normalization(bill_amt)\n",
        "pay_amt_norm = minmax_normalization(pay_amt)\n",
        "\n",
        "\n",
        "# We have a function to create the overlaps - now we need to reshape all the normalized, temporal tensors to be sequences with window overlaps before we concatenate them\n",
        "pay_norm_window = window_tensor(pay_norm, 3, 0.7)\n",
        "bill_amt_norm_window = window_tensor(bill_amt_norm, 3, 0.7)\n",
        "pay_amt_norm_window = window_tensor(pay_amt_norm, 3, 0.7)\n",
        "\n",
        "\n",
        "temporal_data_concatenated = torch.cat((pay_norm_window, bill_amt_norm_window, pay_amt_norm_window), dim=2)\n",
        "\n",
        "z_non_temp_norm_reshaped = z_non_temp_norm.unsqueeze(1).expand(-1, 4, -1)  # Adjust the size of the non-temporal tensor to match the temporal tensors along dimension 1\n",
        "\n",
        "features = torch.cat((z_non_temp_norm_reshaped, temporal_data_concatenated), dim=2)\n",
        "\n",
        "labels_expanded = labels.unsqueeze(1).expand(-1, 4, -1)  # Expand labels to match the features tensor shape\n",
        "\n",
        "input = torch.cat((features, labels_expanded), dim=2).to(torch.float32)\n",
        "\n",
        "print(input.shape)"
      ],
      "metadata": {
        "id": "uJ42Fhu8ZL0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c6901efa-cea7-4987-ea35-87078c1a249a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30000, 4, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom dataset and dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        #return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        features = sample[:, :14]  # Extract features (first 14 columns)\n",
        "        label = sample[:, 14]  # Extract labels (last column)\n",
        "        # return torch.tensor(features), torch.tensor(label)\n",
        "        return features.clone().detach(), label.clone().detach()  # Use clone().detach() to construct tensors from existing data\n",
        "\n",
        "dataset = CustomDataset(input)  # Create a TensorDataset from the data tensor\n",
        "\n",
        "# Define the sizes of your train, test, and validation sets\n",
        "train_size = int(0.7 * len(dataset))  # 70% for training\n",
        "test_size = int(0.15 * len(dataset))  # 15% for testing\n",
        "val_size = len(dataset) - train_size - test_size  # Remaining for validation\n",
        "\n",
        "# Split the dataset into train, test, and validation sets\n",
        "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
        "\n",
        "# Create DataLoader instances for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "oiYRsxtl9OAr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "TRY SUCCESSFULLY APPLYING SMOTE TO TRAINING ONLY\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(input[:, :14], input[:, 14])\n",
        "\n",
        "# Define the custom dataset and dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        #return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        features = sample[:, :14]  # Extract features (first 14 columns)\n",
        "        label = sample[:, 14]  # Extract labels (last column)\n",
        "        # return torch.tensor(features), torch.tensor(label)\n",
        "        return features.clone().detach(), label.clone().detach()  # Use clone().detach() to construct tensors from existing data\n",
        "\n",
        "dataset = CustomDataset(input)  # Create a TensorDataset from the data tensor\n",
        "\n",
        "# Define the sizes of your train, test, and validation sets\n",
        "train_size = int(0.7 * len(dataset))  # 70% for training\n",
        "test_size = int(0.15 * len(dataset))  # 15% for testing\n",
        "val_size = len(dataset) - train_size - test_size  # Remaining for validation\n",
        "\n",
        "# Split the dataset into train, test, and validation sets\n",
        "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
        "\n",
        "# Create DataLoader instances for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)'''"
      ],
      "metadata": {
        "id": "2WDN1jQ_pflQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "d0ebe711-e243-49ff-b28c-9255aae7750e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTRY SUCCESSFULLY APPLYING SMOTE TO TRAINING ONLY\\n\\nfrom imblearn.over_sampling import SMOTE\\nsmote = SMOTE()\\nX_resampled, y_resampled = smote.fit_resample(input[:, :14], input[:, 14])\\n\\n# Define the custom dataset and dataloaders\\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\\n\\nclass CustomDataset(Dataset):\\n    def __init__(self, data):\\n        self.data = data\\n\\n    def __len__(self):\\n        return len(self.data)\\n        #return len(self.features)\\n\\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        features = sample[:, :14]  # Extract features (first 14 columns)\\n        label = sample[:, 14]  # Extract labels (last column)\\n        # return torch.tensor(features), torch.tensor(label)\\n        return features.clone().detach(), label.clone().detach()  # Use clone().detach() to construct tensors from existing data\\n\\ndataset = CustomDataset(input)  # Create a TensorDataset from the data tensor\\n\\n# Define the sizes of your train, test, and validation sets\\ntrain_size = int(0.7 * len(dataset))  # 70% for training\\ntest_size = int(0.15 * len(dataset))  # 15% for testing\\nval_size = len(dataset) - train_size - test_size  # Remaining for validation\\n\\n# Split the dataset into train, test, and validation sets\\ntrain_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\\n\\n# Create DataLoader instances for each set\\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# Set the environment variables for deterministic behavior - important in RNNs as parallelism can result in varied and unexepected results despite having the same inputs and network architecture\n",
        "if torch.version.cuda.startswith('10.1'):\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "elif torch.version.cuda >= '10.2':\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'  # Or ':4096:2' based on your CUDA version\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class CustomRNN(nn.RNN):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, nonlinearity='tanh', bias=True, batch_first=False, dropout=0, bidirectional=False, intermediate_size=30):\n",
        "        super(CustomRNN, self).__init__(input_size, hidden_size, num_layers, nonlinearity, bias, batch_first, dropout, bidirectional)\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(input_size, intermediate_size)\n",
        "        self.linear2 = torch.nn.Linear(intermediate_size, input_size)\n",
        "        self.celu = torch.nn.CELU()\n",
        "        self.gelu = torch.nn.GELU()\n",
        "        self.mish = torch.nn.Mish()\n",
        "        # https://pytorch.org/docs/stable/nn.html ===> use to find different activation functions to use\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        batch_size = input.size(0)\n",
        "        if hx is not None:\n",
        "            if isinstance(hx, tuple):\n",
        "                hx = hx[0]  # Unpack the hidden state tuple into individual tensors\n",
        "        else:\n",
        "            hx = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), batch_size, self.hidden_size).to(input.device)\n",
        "\n",
        "        input = self.mish(self.celu(input))\n",
        "\n",
        "        output, hidden_state = super(CustomRNN, self).forward(input, hx)  # Call the parent class forward method with modified hidden state\n",
        "        return tuple([output, hidden_state])  # unpack this tuple where index 0 is the tensor containing predictions whereas index 1 is the tensor of hidden states after each pass\n",
        "\n",
        "# Define the input parameters\n",
        "input_size = 14\n",
        "hidden_size = 16\n",
        "num_layers = 3\n",
        "\n",
        "# Create an instance of the CustomRNN model\n",
        "model = CustomRNN(input_size, hidden_size, num_layers, 'tanh', False, True, 0.3, True)"
      ],
      "metadata": {
        "id": "10VzNn8U7jhO"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "#define a function for reshaping the targets, as they are tensors of size 64x4 where each array is just a list of the same exact value, i.e. [1,1,1,1] or [0,0,0,0] which isn't necessary\n",
        "def target_reshaper(target: torch.tensor):\n",
        "  return torch.mean(target, dim=1, keepdim=True)\n",
        "\n",
        "#define a function for reshaping the outputs through applying functions\n",
        "def output_reshaper_mean(output: torch.tensor, input: torch.tensor, target: torch.tensor):\n",
        "    outputs_flattened = output.reshape(input.size(0), -1)  # Flatten the outputs\n",
        "    return torch.mean(torch.sigmoid(outputs_flattened), dim=1, keepdim=True)   # Calculate the mean along the second dimension (dim=1) to reduce it to size 64x1\n",
        "\n",
        "def output_reshaper_sum(output: torch.tensor, input: torch.tensor, target: torch.tensor):\n",
        "    outputs_flattened = output.reshape(input.size(0), -1)  # Flatten the outputs\n",
        "    return torch.sum(torch.sigmoid(outputs_flattened) / 100, dim=1, keepdim=True)   # Calculate the sum along the second dimension (dim=1) to reduce it to size 64x1\n",
        "\n",
        "def output_reshaper_std(output: torch.tensor, input: torch.tensor, target: torch.tensor):\n",
        "    outputs_flattened = output.reshape(input.size(0), -1)  # Flatten the outputs\n",
        "    return torch.std(torch.sigmoid(outputs_flattened) / 100, dim=1, keepdim=True)   # Calculate the sum along the second dimension (dim=1) to reduce it to size 64x1\n",
        "\n",
        "def output_reshaper_max(output: torch.tensor, input: torch.tensor, target: torch.tensor):\n",
        "    outputs_flattened = output.reshape(input.size(0), -1)  # Flatten the outputs\n",
        "    return torch.prod(torch.sigmoid(outputs_flattened) / 100, dim=1, keepdim=True)   # Calculate the sum along the second dimension (dim=1) to reduce it to size 64x1\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs, hidden_state = model(inputs)  # Separate outputs and hidden state\n",
        "\n",
        "        outputs = output_reshaper_mean(outputs, inputs, targets)  # reshape to fit the targets --> [64x1]\n",
        "\n",
        "        #outputs.requires_grad = True  # Set requires_grad to True for outputs\n",
        "\n",
        "        #outputs = output_reshaper(outputs, inputs, targets)\n",
        "        targets = target_reshaper(targets)                  # reshape to remove redundant data --> [64x1]\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Initialize variables to store total loss and correct predictions\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    # Iterate over the validation dataset\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs, hidden_state = model(inputs)  # Separate outputs and hidden state\n",
        "            outputs = output_reshaper_mean(outputs, inputs, targets) # reshape to fit the targets\n",
        "            targets = target_reshaper(targets)                  # reshape to remove redundant data\n",
        "\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = (outputs > 0.5).int()  # Assuming binary classification\n",
        "            correct_predictions += (predictions == targets).sum().item()\n",
        "            total_samples += len(targets)\n",
        "\n",
        "    # Calculate metrics for the epoch\n",
        "    epoch_loss = total_loss / len(val_loader)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "\n",
        "    # Print the metrics for the epoch\n",
        "    print(f\"Epoch {epoch + 1}: Validation Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o50geLuLEpF5",
        "outputId": "6e7c91b0-30f3-44f7-c57c-dbb6da696335",
        "collapsed": true
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Loss: 0.5107, Accuracy: 0.7982\n",
            "Epoch 2: Validation Loss: 0.4981, Accuracy: 0.8091\n",
            "Epoch 3: Validation Loss: 0.4926, Accuracy: 0.8144\n",
            "Epoch 4: Validation Loss: 0.4882, Accuracy: 0.8180\n",
            "Epoch 5: Validation Loss: 0.4860, Accuracy: 0.8204\n",
            "Epoch 6: Validation Loss: 0.4855, Accuracy: 0.8227\n",
            "Epoch 7: Validation Loss: 0.4844, Accuracy: 0.8249\n",
            "Epoch 8: Validation Loss: 0.4825, Accuracy: 0.8291\n",
            "Epoch 9: Validation Loss: 0.4840, Accuracy: 0.8240\n",
            "Epoch 10: Validation Loss: 0.4817, Accuracy: 0.8284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store total loss and correct predictions for the test set\n",
        "total_loss_test = 0\n",
        "correct_predictions_test = 0\n",
        "total_samples_test = 0\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Iterate over the test dataset\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "\n",
        "        outputs, hidden_state = model(inputs)  # Separate outputs and hidden state\n",
        "\n",
        "        outputs = output_reshaper_mean(outputs, inputs, targets) # reshape to fit the targets\n",
        "        targets = target_reshaper(targets)                  # reshape to remove redundant data\n",
        "\n",
        "        loss_test = criterion(outputs, targets)\n",
        "        total_loss_test += loss_test.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predictions_test = (outputs > 0.5).int()  # Assuming binary classification\n",
        "        correct_predictions_test += (predictions_test == targets).sum().item()\n",
        "        total_samples_test += len(targets)\n",
        "\n",
        "# Calculate metrics for the test set\n",
        "test_loss = total_loss_test / len(test_loader)\n",
        "test_accuracy = correct_predictions_test / total_samples_test\n",
        "\n",
        "# Print the metrics for the test set\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XKVtjvxcSey",
        "outputId": "a34a88c8-82bd-4687-eb35-e478129f38b4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.4934, Test Accuracy: 0.8136\n"
          ]
        }
      ]
    }
  ]
}